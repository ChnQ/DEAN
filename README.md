<!-- # DEAN: Deactivating the Coupled Neurons to Mitigate Fairness-Privacy Conflicts in Large Language Models -->

<div align="center">
  <h1>DEAN: Deactivating the Coupled Neurons to Mitigate Fairness-Privacy Conflicts in Large Language Models</h1>
  <br />
  <span style="color:red">ğŸ“¢ <strong><i>If you are interested in our work, please star â­ our project.</i></strong></span>

  <h4>
    <a href="https://arxiv.org/abs/2410.16672"><img src="https://img.shields.io/static/v1?label=Paper&message=Arxiv:DEAN&color=red&logo=arxiv"></a>
    <img src="https://img.shields.io/badge/License-Apache_2.0-green.svg" alt="License">
    <img src="https://visitor-badge.laobi.icu/badge?page_id=ChnQ.DEAN" />
    <img src="https://img.shields.io/github/issues/OpenSafetyLab/SALAD-BENCH?color=red" alt="Issues">
  </h4>
</div>


## ğŸŒˆ Introduction


![Overview Diagram](assets/fig1.png)

Ensuring awareness of fairness and privacy in Large Language Models (LLMs) is critical. Interestingly, we discover a counter-intuitive trade-off phenomenon that enhancing an LLM's privacy awareness through Supervised Fine-Tuning (SFT) methods significantly decreases its fairness awareness with thousands of samples. To address this issue, inspired by the information theory, we introduce a training-free method to **DEA**ctivate the fairness and privacy coupled **N**eurons (**DEAN**), which theoretically and empirically decrease the mutual information between fairness and privacy awareness. 

- Extensive experimental results demonstrate that ***DEAN eliminates the trade-off phenomenon and significantly improves LLMs' fairness and privacy awareness simultaneously***, *e.g.*, improving Qwen-2-7B-Instruct's fairness awareness by 12.2\% and privacy awareness by 14.0\%.

- More crucially, ***DEAN remains robust and effective with limited annotated data or even when only malicious fine-tuning data is available***, whereas SFT methods may fail to perform properly in such scenarios. 

We hope this study provides valuable insights into concurrently addressing fairness and privacy concerns in LLMs and can be integrated into comprehensive frameworks to develop more ethical and responsible AI systems.


##  ğŸš©Main Results

<p style="color: #0288D1; font-size: 18px; font-weight: bold;">
    DEAN enhances LLMâ€™s awareness of fairness and privacy simultaneously without compromising general capabilities.
</p>

<!-- DEAN enhances LLMâ€™s awareness of fairness and privacy simultaneously without compromising general capabilities. -->

<img src="./assets/table1.png" width="700" alt="Table 1">

<img src="./assets/table3.png" width="700" alt="Table 3">

<!-- ![Table 1](assets/table1.png) -->

<!-- ![Table 3](assets/table3.png) -->

---
<p style="color: #0288D1; font-size: 18px; font-weight: bold;">
    DEAN remains robust even when only malicious fine-tuning data is available.
</p>

<!-- **DEAN remains robust even when only malicious fine-tuning data is available.** -->

<img src="./assets/fig3.png" width="700" alt="Figure 3">
<!-- ![Figure 3](assets/fig3.png) -->

---

<p style="color: #0288D1; font-size: 18px; font-weight: bold;">
    DEAN encourages the model to produce more cautionary language related to fairness and privacy.
</p>

<!-- **DEAN encourages the model to produce more cautionary language related to fairness and privacy.** -->

<img src="./assets/fig5.png" width="700" alt="Figure 5">

<!-- ![Figure 5](assets/fig5.png) -->


## ğŸš€Quick Start


### ğŸ”§Requirements

The following pakages are required to run the code:

- python==3.11.5

- pytorch==2.1.2

- transformers==4.40.0

- datasets==2.18.0

### ğŸŒŸUsage

**1. Compute and save the importance score**

```bash
cd src/

datasets=(
    "beaver_train330k_privacy_safe_1k"
    "beaver_train330k_fairness_safe_1k"
    "alpaca_cleaned_no_safety"
)

for dataset in "${datasets[@]}"; do
    python compute_importance_score.py \
    --model your_model \
    --model_path your_model_path \
    --nsamples 128 \
    --dataset $dataset
done
```

**2. Run and evaluate DEAN**

```bash
cd src/

python main.py \
--model your_model \
--model_path your_model_path \
--nsamples 128 \
--dataset1 beaver_train330k_privacy_safe_1k \
--dataset2 beaver_train330k_fairness_safe_1k \
--target_module mlp \
--p 5e-7 \
--q 5e-7
```


## ğŸ“License
Distributed under the Apache-2.0 License. See LICENSE for more information.


## Acknowledgements
Some code in this project is adapted from resources provided by the following repositories: 
- https://github.com/boyiwei/alignment-attribution-code
- https://github.com/OpenSafetyLab/SALAD-BENCH

We greatly appreciate the contributions of the original authors.


## ğŸ“–BibTeX
```
@article{qian2024dean,
  title={DEAN: Deactivating the Coupled Neurons to Mitigate Fairness-Privacy Conflicts in Large Language Models},
  author={Qian, Chen and Liu, Dongrui and Zhang, Jie and Liu, Yong and Shao, Jing},
  journal={arXiv preprint arXiv:2410.16672},
  year={2024}
}